{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud # for wordcloud\n",
    "import matplotlib.pyplot as plt # for wordcloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, hamming_loss, f1_score, jaccard_score, accuracy_score\n",
    "from itertools import product\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a more balance train data\n",
    "train_data = pd.read_csv('updated_more_balanced_data.csv') # obtain balance data from the first 600 data manually\n",
    "test_data = pd.read_csv('first_600_manually_add_theme.csv')\n",
    "\n",
    "test_x = test_data[\"summary\"]\n",
    "test_y = test_data[\"theme\"]\n",
    "\n",
    "train_x = train_data[\"summary\"]\n",
    "train_y = train_data[\"theme\"]\n",
    "\n",
    "categories = [\"corporate and business topics\", \n",
    "              \"labor and employment issues\", \n",
    "              \"privacy, security, and cyber matters\", \n",
    "              \"legal and crime stories\", \n",
    "              \"government actions and regulations\", \n",
    "              \"technology and digital trends\", \n",
    "              \"environment and climate topics\", \n",
    "              \"social issues and activism\", \n",
    "              \"healthcare and medicine\", \n",
    "              \"community and cultural events\", \n",
    "              \"international relations and trade\", \n",
    "              \"education and learning\", \n",
    "              \"consumer topics\", \n",
    "              \"infrastructure and development\", \n",
    "              \"energy and resources\", \n",
    "              \"political topics and protests\", \n",
    "              \"media and communication\", \n",
    "              \"financial policies and taxation\", \n",
    "              \"human rights and social justice\", \n",
    "              \"science, research, and innovation\", \n",
    "              \"disaster and crisis management\", \n",
    "              \"organized crime and trafficking\", \n",
    "              \"sports, entertainment, and leisure\", \n",
    "              \"other\", \n",
    "              \"military\"]\n",
    "\n",
    "y_encoded = []\n",
    "for each_theme in train_y:\n",
    "    each_row = []\n",
    "    for category in categories:\n",
    "        if category in each_theme.lower():\n",
    "            each_row.append(1)\n",
    "        else:\n",
    "            each_row.append(0)\n",
    "    y_encoded.append(each_row)\n",
    "\n",
    "# convert to dataframe\n",
    "y_encoded = pd.DataFrame(y_encoded, columns = categories)\n",
    "train_y = y_encoded\n",
    "\n",
    "y_encoded = []\n",
    "for each_theme in test_y:\n",
    "    each_row = []\n",
    "    for category in categories:\n",
    "        if category in each_theme.lower():\n",
    "            each_row.append(1)\n",
    "        else:\n",
    "            each_row.append(0)\n",
    "    y_encoded.append(each_row)\n",
    "\n",
    "y_encoded = pd.DataFrame(y_encoded, columns = categories)\n",
    "test_y = y_encoded   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION - WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all the sentences\n",
    "tokenized_sentences = [word_tokenize(each_line[0].lower()) for each_line in train_data[\"summary\"]]\n",
    "\n",
    "# train word2vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences = tokenized_sentences, \n",
    "    vector_size = 1000, \n",
    "    window = 5, \n",
    "    min_count = 1, \n",
    "    workers = 4\n",
    ")\n",
    "\n",
    "# generate document vectors\n",
    "def vectorize_doc(each_line):\n",
    "    # remove out of vocab words\n",
    "    words = [word for word in each_line if word in word2vec_model.wv]\n",
    "    return np.mean(word2vec_model.wv[words], axis = 0) if words else np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# create feature vectors \n",
    "train_x = np.array([vectorize_doc(word_tokenize(each_line.lower())) for each_line in train_data[\"summary\"]])\n",
    "test_x = np.array([vectorize_doc(word_tokenize(each_line.lower())) for each_line in test_data[\"summary\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning using Grid Search\n",
    "\n",
    "need to tune:\n",
    "\n",
    "vector_size, window, min_count, sg, hs and negative, epochs, alpha and min_alpha, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.04257029768172584\n",
      "100 3 1 0 0\n"
     ]
    }
   ],
   "source": [
    "vector_sizes = [100, 200, 300]\n",
    "windows = [3, 5, 7, 10]\n",
    "min_counts = [1, 2, 3, 4, 5]\n",
    "sgs = [0, 1]\n",
    "hs = [0, 1]\n",
    "\n",
    "tuned_model = None\n",
    "tuned_score = -float('inf')\n",
    "\n",
    "\n",
    "highest = 0\n",
    "for vector_size, window, min_count, sg, each_hs in product(vector_sizes, windows, min_counts, sgs, hs):\n",
    "    model = Word2Vec(train_data[\"summary\"], \n",
    "                      vector_size = vector_size, \n",
    "                      window = window, \n",
    "                      min_count = min_count, \n",
    "                      sg = sg, \n",
    "                      hs = each_hs\n",
    "                      )\n",
    "    \n",
    "    # create feature vectors \n",
    "    train_x = np.array([vectorize_doc(word_tokenize(each_line.lower())) for each_line in train_data[\"summary\"]])\n",
    "    test_x = np.array([vectorize_doc(word_tokenize(each_line.lower())) for each_line in test_data[\"summary\"]])\n",
    "\n",
    "    # label powerset\n",
    "    # it can capture label dependencies but if too many label combinations\n",
    "    # it will lead to poor performance\n",
    "\n",
    "\n",
    "\n",
    "    model = LabelPowerset(RandomForestClassifier(n_estimators = 100, \n",
    "                                                random_state = 59))\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    if accuracy_score(test_y, pred) > highest:\n",
    "        highest = accuracy_score(test_y, pred)\n",
    "        print(\"Accuracy: \", highest)\n",
    "        print(vector_size, window, min_count, sg, each_hs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics do not change throughout the all the tuning. Hence, the current hyperparameters do not need to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\axlee\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (4, 18)\t1\n",
      "  (5, 2)\t1\n",
      "  (6, 2)\t1\n",
      "  (7, 21)\t1\n",
      "  (8, 2)\t1\n",
      "  (9, 2)\t1\n",
      "  (10, 2)\t1\n",
      "  (11, 2)\t1\n",
      "  (12, 2)\t1\n",
      "  (13, 2)\t1\n",
      "  (14, 3)\t1\n",
      "  (15, 2)\t1\n",
      "  (16, 2)\t1\n",
      "  (17, 2)\t1\n",
      "  (18, 21)\t1\n",
      "  (19, 2)\t1\n",
      "  (20, 2)\t1\n",
      "  (21, 2)\t1\n",
      "  (22, 2)\t1\n",
      "  (23, 2)\t1\n",
      "  (24, 2)\t1\n",
      "  (25, 2)\t1\n",
      "  (26, 18)\t1\n",
      "  (27, 12)\t1\n",
      "  (28, 2)\t1\n",
      "  (29, 0)\t1\n",
      "  (29, 5)\t1\n",
      "  (30, 2)\t1\n",
      "  (31, 2)\t1\n",
      "  (32, 2)\t1\n",
      "  (33, 2)\t1\n",
      "  (34, 2)\t1\n",
      "  (35, 2)\t1\n",
      "  (36, 2)\t1\n",
      "  (37, 2)\t1\n",
      "  (38, 5)\t1\n",
      "  (39, 2)\t1\n",
      "  (40, 21)\t1\n",
      "  (41, 5)\t1\n",
      "  (42, 18)\t1\n",
      "  (43, 2)\t1\n",
      "  (44, 2)\t1\n",
      "  (45, 2)\t1\n",
      "  (46, 18)\t1\n",
      "  (47, 2)\t1\n",
      "  (48, 2)\t1\n",
      "  (49, 18)\t1\n",
      "  (50, 2)\t1\n",
      "  (51, 2)\t1\n",
      "  (52, 18)\t1\n",
      "  (53, 18)\t1\n",
      "  (54, 2)\t1\n",
      "  (55, 2)\t1\n",
      "  (56, 21)\t1\n",
      "  (57, 18)\t1\n",
      "  (58, 18)\t1\n",
      "  (59, 2)\t1\n",
      "  (60, 2)\t1\n",
      "  (61, 18)\t1\n",
      "  (62, 18)\t1\n",
      "  (63, 2)\t1\n",
      "  (64, 18)\t1\n",
      "  (65, 2)\t1\n",
      "  (66, 2)\t1\n",
      "  (67, 18)\t1\n",
      "  (68, 18)\t1\n",
      "  (69, 2)\t1\n",
      "  (70, 18)\t1\n",
      "  (71, 2)\t1\n",
      "  (72, 2)\t1\n",
      "  (73, 18)\t1\n",
      "  (74, 2)\t1\n",
      "  (75, 18)\t1\n",
      "  (76, 21)\t1\n",
      "  (77, 18)\t1\n",
      "  (78, 2)\t1\n",
      "  (79, 2)\t1\n",
      "  (80, 2)\t1\n",
      "  (81, 12)\t1\n",
      "  (82, 18)\t1\n",
      "  (83, 2)\t1\n",
      "  (84, 18)\t1\n",
      "  (85, 18)\t1\n",
      "  (86, 2)\t1\n",
      "  (87, 18)\t1\n",
      "  (88, 2)\t1\n",
      "  (89, 18)\t1\n",
      "  (90, 2)\t1\n",
      "  (91, 18)\t1\n",
      "  (92, 2)\t1\n",
      "  (93, 2)\t1\n",
      "  (94, 18)\t1\n",
      "  (95, 2)\t1\n",
      "  (96, 2)\t1\n",
      "  (97, 12)\t1\n",
      "  (98, 2)\t1\n",
      "  (99, 18)\t1\n",
      "  (100, 18)\t1\n",
      "  (101, 18)\t1\n",
      "  (102, 2)\t1\n",
      "  (103, 18)\t1\n",
      "  (104, 18)\t1\n",
      "  (105, 2)\t1\n",
      "  (106, 18)\t1\n",
      "  (107, 2)\t1\n",
      "  (108, 18)\t1\n",
      "  (109, 18)\t1\n",
      "  (110, 18)\t1\n",
      "  (111, 2)\t1\n",
      "  (112, 18)\t1\n",
      "  (113, 2)\t1\n",
      "  (114, 18)\t1\n",
      "  (115, 2)\t1\n",
      "  (116, 18)\t1\n",
      "  (117, 18)\t1\n",
      "  (118, 2)\t1\n",
      "  (119, 2)\t1\n",
      "  (120, 2)\t1\n",
      "  (121, 18)\t1\n",
      "  (122, 2)\t1\n",
      "  (123, 2)\t1\n",
      "  (124, 18)\t1\n",
      "  (125, 18)\t1\n",
      "  (126, 2)\t1\n",
      "  (127, 2)\t1\n",
      "  (128, 2)\t1\n",
      "  (129, 2)\t1\n",
      "  (130, 2)\t1\n",
      "  (131, 2)\t1\n",
      "  (132, 2)\t1\n",
      "  (133, 2)\t1\n",
      "  (134, 2)\t1\n",
      "  (135, 18)\t1\n",
      "  (136, 2)\t1\n",
      "  (137, 2)\t1\n",
      "  (138, 2)\t1\n",
      "  (139, 22)\t1\n",
      "  (140, 22)\t1\n",
      "  (141, 2)\t1\n",
      "  (142, 2)\t1\n",
      "  (143, 3)\t1\n",
      "  (143, 24)\t1\n",
      "  (144, 2)\t1\n",
      "  (145, 18)\t1\n",
      "  (146, 18)\t1\n",
      "  (147, 18)\t1\n",
      "  (148, 18)\t1\n",
      "  (149, 2)\t1\n",
      "  (150, 18)\t1\n",
      "  (151, 18)\t1\n",
      "  (152, 5)\t1\n",
      "  (153, 2)\t1\n",
      "  (154, 18)\t1\n",
      "  (155, 2)\t1\n",
      "  (156, 2)\t1\n",
      "  (157, 2)\t1\n",
      "  (158, 18)\t1\n",
      "  (159, 2)\t1\n",
      "  (160, 0)\t1\n",
      "  (160, 5)\t1\n",
      "  (161, 2)\t1\n",
      "  (162, 2)\t1\n",
      "  (163, 2)\t1\n",
      "  (164, 2)\t1\n",
      "  (165, 2)\t1\n",
      "  (166, 18)\t1\n",
      "  (167, 21)\t1\n",
      "  (168, 2)\t1\n",
      "  (169, 2)\t1\n",
      "  (170, 2)\t1\n",
      "  (171, 2)\t1\n",
      "  (172, 18)\t1\n",
      "  (173, 2)\t1\n",
      "  (174, 2)\t1\n",
      "  (175, 2)\t1\n",
      "  (176, 18)\t1\n",
      "  (177, 18)\t1\n",
      "  (178, 2)\t1\n",
      "  (179, 18)\t1\n",
      "  (180, 18)\t1\n",
      "  (181, 2)\t1\n",
      "  (182, 2)\t1\n",
      "  (183, 2)\t1\n",
      "  (184, 2)\t1\n",
      "  (185, 18)\t1\n",
      "  (186, 18)\t1\n",
      "  (187, 18)\t1\n",
      "  (188, 22)\t1\n",
      "  (189, 2)\t1\n",
      "  (190, 2)\t1\n",
      "  (191, 18)\t1\n",
      "  (192, 12)\t1\n",
      "  (193, 2)\t1\n",
      "  (194, 18)\t1\n",
      "  (195, 2)\t1\n",
      "  (196, 2)\t1\n",
      "  (197, 2)\t1\n",
      "  (198, 18)\t1\n",
      "  (199, 18)\t1\n",
      "  (200, 18)\t1\n",
      "  (201, 2)\t1\n",
      "  (202, 18)\t1\n",
      "  (203, 2)\t1\n",
      "  (204, 18)\t1\n",
      "  (205, 5)\t1\n",
      "  (206, 2)\t1\n",
      "  (207, 2)\t1\n",
      "  (208, 0)\t1\n",
      "  (208, 5)\t1\n",
      "  (209, 2)\t1\n",
      "  (210, 2)\t1\n",
      "  (211, 2)\t1\n",
      "  (212, 18)\t1\n",
      "  (213, 2)\t1\n",
      "  (214, 2)\t1\n",
      "  (215, 2)\t1\n",
      "  (216, 18)\t1\n",
      "  (217, 2)\t1\n",
      "  (218, 18)\t1\n",
      "  (219, 18)\t1\n",
      "  (220, 2)\t1\n",
      "  (221, 18)\t1\n",
      "  (222, 2)\t1\n",
      "  (223, 2)\t1\n",
      "  (224, 2)\t1\n",
      "  (225, 2)\t1\n",
      "  (226, 2)\t1\n",
      "  (227, 2)\t1\n",
      "  (228, 2)\t1\n",
      "  (229, 2)\t1\n",
      "  (230, 2)\t1\n",
      "  (231, 2)\t1\n",
      "  (232, 18)\t1\n",
      "  (233, 18)\t1\n",
      "  (234, 2)\t1\n",
      "  (235, 2)\t1\n",
      "  (236, 2)\t1\n",
      "  (237, 2)\t1\n",
      "  (238, 18)\t1\n",
      "  (239, 18)\t1\n",
      "  (240, 2)\t1\n",
      "  (241, 5)\t1\n",
      "  (242, 18)\t1\n",
      "  (243, 18)\t1\n",
      "  (244, 18)\t1\n",
      "  (245, 3)\t1\n",
      "  (246, 18)\t1\n",
      "  (247, 2)\t1\n",
      "  (248, 18)\t1\n",
      "  (249, 5)\t1\n",
      "  (250, 2)\t1\n",
      "  (251, 2)\t1\n",
      "  (252, 18)\t1\n",
      "  (253, 2)\t1\n",
      "  (254, 18)\t1\n",
      "  (255, 18)\t1\n",
      "  (256, 2)\t1\n",
      "  (257, 2)\t1\n",
      "  (258, 18)\t1\n",
      "  (259, 18)\t1\n",
      "  (260, 2)\t1\n",
      "  (261, 18)\t1\n",
      "  (262, 2)\t1\n",
      "  (263, 2)\t1\n",
      "  (264, 3)\t1\n",
      "  (265, 5)\t1\n",
      "  (266, 18)\t1\n",
      "  (267, 3)\t1\n",
      "  (268, 18)\t1\n",
      "  (269, 18)\t1\n",
      "  (270, 2)\t1\n",
      "  (271, 3)\t1\n",
      "  (272, 2)\t1\n",
      "  (273, 2)\t1\n",
      "  (274, 2)\t1\n",
      "  (275, 2)\t1\n",
      "  (276, 18)\t1\n",
      "  (277, 2)\t1\n",
      "  (278, 2)\t1\n",
      "  (279, 18)\t1\n",
      "  (280, 18)\t1\n",
      "  (281, 18)\t1\n",
      "  (282, 12)\t1\n",
      "  (283, 18)\t1\n",
      "  (284, 5)\t1\n",
      "  (285, 2)\t1\n",
      "  (286, 2)\t1\n",
      "  (287, 5)\t1\n",
      "  (288, 2)\t1\n",
      "  (289, 18)\t1\n",
      "  (290, 0)\t1\n",
      "  (290, 5)\t1\n",
      "  (291, 18)\t1\n",
      "  (292, 2)\t1\n",
      "  (293, 18)\t1\n",
      "  (294, 0)\t1\n",
      "  (294, 5)\t1\n",
      "  (295, 2)\t1\n",
      "  (296, 2)\t1\n",
      "  (297, 18)\t1\n",
      "  (298, 2)\t1\n",
      "  (299, 18)\t1\n",
      "  (300, 21)\t1\n",
      "  (301, 3)\t1\n",
      "  (302, 2)\t1\n",
      "  (303, 18)\t1\n",
      "  (304, 2)\t1\n",
      "  (305, 2)\t1\n",
      "  (306, 18)\t1\n",
      "  (307, 5)\t1\n",
      "  (308, 18)\t1\n",
      "  (309, 2)\t1\n",
      "  (310, 2)\t1\n",
      "  (311, 18)\t1\n",
      "  (312, 18)\t1\n",
      "  (313, 18)\t1\n",
      "  (314, 18)\t1\n",
      "  (315, 0)\t1\n",
      "  (315, 5)\t1\n",
      "  (316, 2)\t1\n",
      "  (317, 2)\t1\n",
      "  (318, 2)\t1\n",
      "  (319, 2)\t1\n",
      "  (320, 2)\t1\n",
      "  (321, 12)\t1\n",
      "  (322, 18)\t1\n",
      "  (323, 18)\t1\n",
      "  (324, 18)\t1\n",
      "  (325, 18)\t1\n",
      "  (326, 2)\t1\n",
      "  (327, 2)\t1\n",
      "  (328, 2)\t1\n",
      "  (329, 21)\t1\n",
      "  (330, 18)\t1\n",
      "  (331, 2)\t1\n",
      "  (332, 2)\t1\n",
      "  (333, 18)\t1\n",
      "  (334, 18)\t1\n",
      "  (335, 2)\t1\n",
      "  (336, 18)\t1\n",
      "  (337, 2)\t1\n",
      "  (338, 2)\t1\n",
      "  (339, 2)\t1\n",
      "  (340, 2)\t1\n",
      "  (341, 18)\t1\n",
      "  (342, 2)\t1\n",
      "  (343, 0)\t1\n",
      "  (343, 5)\t1\n",
      "  (344, 18)\t1\n",
      "  (345, 2)\t1\n",
      "  (346, 2)\t1\n",
      "  (347, 2)\t1\n",
      "  (348, 12)\t1\n",
      "  (349, 0)\t1\n",
      "  (349, 5)\t1\n",
      "  (350, 2)\t1\n",
      "  (351, 0)\t1\n",
      "  (351, 5)\t1\n",
      "  (352, 18)\t1\n",
      "  (353, 2)\t1\n",
      "  (354, 2)\t1\n",
      "  (355, 18)\t1\n",
      "  (356, 2)\t1\n",
      "  (357, 18)\t1\n",
      "  (358, 2)\t1\n",
      "  (359, 2)\t1\n",
      "  (360, 2)\t1\n",
      "  (361, 18)\t1\n",
      "  (362, 2)\t1\n",
      "  (363, 18)\t1\n",
      "  (364, 2)\t1\n",
      "  (365, 18)\t1\n",
      "  (366, 18)\t1\n",
      "  (367, 2)\t1\n",
      "  (368, 2)\t1\n",
      "  (369, 18)\t1\n",
      "  (370, 5)\t1\n",
      "  (371, 2)\t1\n",
      "  (372, 2)\t1\n",
      "  (373, 2)\t1\n",
      "  (374, 2)\t1\n",
      "  (375, 2)\t1\n",
      "  (376, 2)\t1\n",
      "  (377, 2)\t1\n",
      "  (378, 2)\t1\n",
      "  (379, 2)\t1\n",
      "  (380, 2)\t1\n",
      "  (381, 18)\t1\n",
      "  (382, 18)\t1\n",
      "  (383, 2)\t1\n",
      "  (384, 2)\t1\n",
      "  (385, 18)\t1\n",
      "  (386, 18)\t1\n",
      "  (387, 18)\t1\n",
      "  (388, 2)\t1\n",
      "  (389, 2)\t1\n",
      "  (390, 12)\t1\n",
      "  (391, 12)\t1\n",
      "  (392, 18)\t1\n",
      "  (393, 12)\t1\n",
      "  (394, 2)\t1\n",
      "  (395, 2)\t1\n",
      "  (396, 2)\t1\n",
      "  (397, 2)\t1\n",
      "  (398, 2)\t1\n",
      "  (399, 2)\t1\n",
      "  (400, 2)\t1\n",
      "  (401, 2)\t1\n",
      "  (402, 2)\t1\n",
      "  (403, 2)\t1\n",
      "  (404, 18)\t1\n",
      "  (405, 2)\t1\n",
      "  (406, 2)\t1\n",
      "  (407, 18)\t1\n",
      "  (408, 18)\t1\n",
      "  (409, 2)\t1\n",
      "  (410, 18)\t1\n",
      "  (411, 2)\t1\n",
      "  (412, 2)\t1\n",
      "  (413, 2)\t1\n",
      "  (414, 2)\t1\n",
      "  (415, 2)\t1\n",
      "  (416, 2)\t1\n",
      "  (417, 18)\t1\n",
      "  (418, 2)\t1\n",
      "  (419, 2)\t1\n",
      "  (420, 2)\t1\n",
      "  (421, 18)\t1\n",
      "  (422, 2)\t1\n",
      "  (423, 18)\t1\n",
      "  (424, 2)\t1\n",
      "  (425, 18)\t1\n",
      "  (426, 2)\t1\n",
      "  (427, 2)\t1\n",
      "  (428, 2)\t1\n",
      "  (429, 18)\t1\n",
      "  (430, 18)\t1\n",
      "  (431, 18)\t1\n",
      "  (432, 2)\t1\n",
      "  (433, 2)\t1\n",
      "  (434, 2)\t1\n",
      "  (435, 0)\t1\n",
      "  (435, 5)\t1\n",
      "  (436, 2)\t1\n",
      "  (437, 2)\t1\n",
      "  (438, 18)\t1\n",
      "  (439, 2)\t1\n",
      "  (440, 18)\t1\n",
      "  (441, 2)\t1\n",
      "  (442, 2)\t1\n",
      "  (443, 0)\t1\n",
      "  (443, 5)\t1\n",
      "  (444, 2)\t1\n",
      "  (445, 18)\t1\n",
      "  (446, 18)\t1\n",
      "  (447, 2)\t1\n",
      "  (448, 2)\t1\n",
      "  (449, 2)\t1\n",
      "  (450, 18)\t1\n",
      "  (451, 5)\t1\n",
      "  (452, 18)\t1\n",
      "  (453, 2)\t1\n",
      "  (454, 2)\t1\n",
      "  (455, 2)\t1\n",
      "  (456, 2)\t1\n",
      "  (457, 2)\t1\n",
      "  (458, 2)\t1\n",
      "  (459, 2)\t1\n",
      "  (460, 2)\t1\n",
      "  (461, 2)\t1\n",
      "  (462, 2)\t1\n",
      "  (463, 2)\t1\n",
      "  (464, 18)\t1\n",
      "  (465, 2)\t1\n",
      "  (466, 2)\t1\n",
      "  (467, 18)\t1\n",
      "  (468, 18)\t1\n",
      "  (469, 18)\t1\n",
      "  (470, 18)\t1\n",
      "  (471, 2)\t1\n",
      "  (472, 18)\t1\n",
      "  (473, 2)\t1\n",
      "  (474, 18)\t1\n",
      "  (475, 18)\t1\n",
      "  (476, 2)\t1\n",
      "  (477, 2)\t1\n",
      "  (478, 2)\t1\n",
      "  (479, 2)\t1\n",
      "  (480, 2)\t1\n",
      "  (481, 2)\t1\n",
      "  (482, 18)\t1\n",
      "  (483, 18)\t1\n",
      "  (484, 2)\t1\n",
      "  (485, 18)\t1\n",
      "  (486, 2)\t1\n",
      "  (487, 2)\t1\n",
      "  (488, 2)\t1\n",
      "  (489, 18)\t1\n",
      "  (490, 18)\t1\n",
      "  (491, 2)\t1\n",
      "  (492, 2)\t1\n",
      "  (493, 18)\t1\n",
      "  (494, 18)\t1\n",
      "  (495, 18)\t1\n",
      "  (496, 12)\t1\n",
      "  (497, 18)\t1\n",
      "  (498, 18)\t1\n",
      "  (499, 2)\t1\n",
      "  (500, 3)\t1\n",
      "  (501, 18)\t1\n",
      "  (502, 2)\t1\n",
      "  (503, 18)\t1\n",
      "  (504, 18)\t1\n",
      "  (505, 2)\t1\n",
      "  (506, 2)\t1\n",
      "  (507, 2)\t1\n",
      "  (508, 2)\t1\n",
      "  (509, 2)\t1\n",
      "  (510, 2)\t1\n",
      "  (511, 2)\t1\n",
      "  (512, 2)\t1\n",
      "  (513, 18)\t1\n",
      "  (514, 2)\t1\n",
      "  (515, 18)\t1\n",
      "  (516, 2)\t1\n",
      "  (517, 2)\t1\n",
      "  (518, 2)\t1\n",
      "  (519, 18)\t1\n",
      "  (520, 2)\t1\n",
      "  (521, 2)\t1\n",
      "  (522, 18)\t1\n",
      "  (523, 18)\t1\n",
      "  (524, 2)\t1\n",
      "  (525, 18)\t1\n",
      "  (526, 2)\t1\n",
      "  (527, 2)\t1\n",
      "  (528, 2)\t1\n",
      "  (529, 2)\t1\n",
      "  (530, 18)\t1\n",
      "  (531, 2)\t1\n",
      "  (532, 2)\t1\n",
      "  (533, 2)\t1\n",
      "  (534, 2)\t1\n",
      "  (535, 18)\t1\n",
      "  (536, 2)\t1\n",
      "  (537, 2)\t1\n",
      "  (538, 2)\t1\n",
      "  (539, 18)\t1\n",
      "  (540, 2)\t1\n",
      "  (541, 2)\t1\n",
      "  (542, 12)\t1\n",
      "  (543, 18)\t1\n",
      "  (544, 2)\t1\n",
      "  (545, 18)\t1\n",
      "  (546, 2)\t1\n",
      "  (547, 12)\t1\n",
      "  (548, 18)\t1\n",
      "  (549, 12)\t1\n",
      "  (550, 2)\t1\n",
      "  (551, 18)\t1\n",
      "  (552, 2)\t1\n",
      "  (553, 2)\t1\n",
      "  (554, 2)\t1\n",
      "  (555, 2)\t1\n",
      "  (556, 2)\t1\n",
      "  (557, 2)\t1\n",
      "  (558, 2)\t1\n",
      "  (559, 18)\t1\n",
      "  (560, 2)\t1\n",
      "  (561, 2)\t1\n",
      "  (562, 2)\t1\n",
      "  (563, 18)\t1\n",
      "  (564, 2)\t1\n",
      "  (565, 2)\t1\n",
      "  (566, 2)\t1\n",
      "  (567, 18)\t1\n",
      "  (568, 18)\t1\n",
      "  (569, 2)\t1\n",
      "  (570, 2)\t1\n",
      "  (571, 18)\t1\n",
      "  (572, 18)\t1\n",
      "  (573, 0)\t1\n",
      "  (573, 5)\t1\n",
      "  (574, 2)\t1\n",
      "  (575, 3)\t1\n",
      "  (576, 2)\t1\n",
      "  (577, 18)\t1\n",
      "  (578, 2)\t1\n",
      "  (579, 18)\t1\n",
      "  (580, 2)\t1\n",
      "  (581, 0)\t1\n",
      "  (581, 5)\t1\n",
      "  (582, 2)\t1\n",
      "  (583, 2)\t1\n",
      "  (584, 2)\t1\n",
      "  (585, 2)\t1\n",
      "  (586, 18)\t1\n",
      "  (587, 2)\t1\n",
      "  (588, 2)\t1\n",
      "  (589, 18)\t1\n",
      "  (590, 18)\t1\n",
      "  (591, 18)\t1\n",
      "  (592, 18)\t1\n",
      "  (593, 18)\t1\n",
      "  (594, 2)\t1\n",
      "  (595, 18)\t1\n",
      "  (596, 2)\t1\n",
      "  (597, 18)\t1\n",
      "  (598, 2)\t1\n",
      "  (599, 2)\t1\n",
      "best parameters:  {'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "base_model = LabelPowerset(RandomForestClassifier(n_estimators = 100,\n",
    "                                                  max_depth = 25, \n",
    "                                                  min_samples_split = 5, \n",
    "                                                  min_samples_leaf = 2,  \n",
    "                                                  random_state = 59))\n",
    "\n",
    "parameters = {\n",
    "    'classifier__n_estimators': [50, 100, 200], \n",
    "    'classifier__max_depth': [None, 10, 20], \n",
    "    'classifier__min_samples_split': [2, 5, 10], \n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "} \n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    base_model, \n",
    "    parameters,\n",
    "    scoring = make_scorer(\n",
    "        f1_score, \n",
    "        average = 'micro'), \n",
    "        cv = 3,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "grid_search.fit(train_x, train_y)\n",
    "print(grid_search.predict(test_x))\n",
    "print(\"best parameters: \", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
